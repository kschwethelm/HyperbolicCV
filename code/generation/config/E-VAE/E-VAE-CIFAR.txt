# python code/generation/train.py -c generation/config/E-VAE/E-VAE-CIFAR.txt

# Output settings
exp_name = E-VAE-CIFAR-10
#output_dir = generation/output

# General settings
device = cuda:0
dtype = float32
seed = 1
#load_checkpoint = generation/output/E-VAE-CIFAR-10

# General training hyperparameters
num_epochs = 100
batch_size = 100
lr = 5e-4
weight_decay = 0
optimizer = Adam
use_lr_scheduler = True
lr_scheduler_step = 50
lr_scheduler_gamma = 0.1

# Embeding Loss hyperparameters
kl_coeff = 0.024

# General validation/testing hyperparameters
batch_size_test = 1024
calc_fid_val = True
batch_size_fid_inception = 512

# General VAE hyperparameters
model = E-VAE
enc_layers = 4
dec_layers = 3
z_dim = 128
initial_filters = 64

# Dataset settings
dataset = CIFAR-10 # or CIFAR-100
